{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkToQ5o/bm8wob/aHavum5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeedrajpoot1/project02-RAG/blob/main/project02RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujZH0pYW6Xqk",
        "outputId": "0e8456fe-a8c0-46e5-8cba-47a3dc2c25f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.1/1.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain_pinecone langchain_community pinecone-client langchain-google-genai openai tqdm pypdf PyPDF2 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BMMBgY_F699n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh4FSACL7qk2",
        "outputId": "dc300e4f-7c20-44ca-88f4-c0a5db5c1bf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"PINECONE_API_KEY\"]: str = \"pcsk_6CL5u7_DDnhKvcCdTWgvtpoqH4gQS4nYKFh2gYitcHNgCp8QqwJhRfxfyjxpP1qLa5oNS3\"\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"]: str = \"AIzaSyC-dqxwAtOX8lsX2Cr3iANLdGhYs2tT030\"\n",
        "\n",
        "# Access the environment variable\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "GOOGLE_API_KEY= os.getenv(\"GOOGLE_API_KEY\")\n",
        "print(PINECONE_API_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxWpfHbG8lSv",
        "outputId": "c614f90b-42fa-45a0-85e6-e7700486cbea"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pcsk_6CL5u7_DDnhKvcCdTWgvtpoqH4gQS4nYKFh2gYitcHNgCp8QqwJhRfxfyjxpP1qLa5oNS3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this index name is alrrady saved thats why this is showing this line red\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "index_name = \"rag\"\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768, # Replace with your model dimensions\n",
        "    metric=\"cosine\", # Replace with your model metric\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "index=pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "Ff-1d9uf9_9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "\n",
        "embeddings = GooglePalmEmbeddings(google_api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "Y5ZPnLcPBz4l"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=GOOGLE_API_KEY,model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "dA1VgkhnCG5Y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store=PineconeVectorStore(index=index,embedding=embeddings)"
      ],
      "metadata": {
        "id": "EEXCbG4nixNH"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"generative_ai_info.txt\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(docs[0])  # Print the first document to see its structure\n",
        "print(dir(docs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioDrdkv6CVnf",
        "outputId": "282caa5c-f98b-4e44-b018-f5df114a7988"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Generative AI Information File' metadata={'source': 'generative_ai_info.txt'}\n",
            "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'cast_id_to_str', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'id', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqT_LL--jhhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    metadata = {\"source\": doc.metadata[\"source\"]}  # Ensure metadata is a dictionary\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, metadata)])  # Pass metadata as a dictionary\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68vW03yCDLFC",
        "outputId": "33bfd635-28cd-4a80-9233-30a8ea14723b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 141/141 [00:38<00:00,  3.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "\n",
        "# Create the retriever object with the correct parameters\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "# Create the retriever object with only the required arguments\n",
        "retriever = Pinecone(index=index, embedding=embeddings,text_key=\"text\")\n",
        "retriever = VectorStoreRetriever(vectorstore=retriever)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yqQVBC3Gm00u"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY,model='gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "m3-3Ox8voeDy"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "HcjINZZgpDdz"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the history of artificial intelligence?\"\n",
        "response = qa_chain.invoke(query)\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euk_CVAOqc82",
        "outputId": "beae87c6-628b-40b6-8a49-fa26971ad54c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.  The provided text focuses on applications, ethical concerns, and the future of generative AI, not its history.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"can you tell in a short about this document?\"\n",
        "response = qa_chain.invoke(query)\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8urdg8lqrir",
        "outputId": "2b0223f4-43db-4e8a-f4ca-6e0dddd257cb"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.pinecone:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses ethical concerns and the future of generative AI.  Ethical concerns include bias in training data, the misuse of deepfakes, and copyright issues.  The future of generative AI is predicted to involve significant transformation across industries, combining creativity and computation.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}